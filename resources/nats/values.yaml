####################################
#                                  #
#       Global Configuration       #
#                                  #
####################################

global:
  containerRegistry:
    path: europe-docker.pkg.dev/kyma-project
  images:
    nats:
      name: nats
      version: v20240102-2.10.7-alpine3.18
      directory: prod/external
    nats_config_reloader:
      name: natsio/nats-server-config-reloader
      version: 0.14.1
      directory: prod/external
    prometheus_nats_exporter:
      name: natsio/prometheus-nats-exporter
      version: 0.14.0
      directory: prod/external
    alpine:
      name: alpine
      version: 3.19.1
      directory: prod/external

  jetstream:
    # Storage type of the stream, memory or file.
    storage: file
    fileStorage:
      size: 1Gi
    # podManagementPolicy controls how pods are created during initial scale up,
    # when replacing pods on nodes, or when scaling down.
    podManagementPolicy: Parallel

  priorityClassName: "nats-manager-priority-class"

####################################
#                                  #
#  Security Context Configuration  #
#                                  #
####################################
podSecurityContext:
  runAsUser: 1000
  fsGroup: 10001
  seccompProfile:
    type: RuntimeDefault

containerSecurityContext:
  privileged: false
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL

###############################
#                             #
#     Istio Configuration     #
#                             #
###############################
istio:
  enabled: true

###############################
#                             #
#  NATS Server Configuration  #
#                             #
###############################
nats:
  pullPolicy: IfNotPresent
  ports:
    client: 4222
    monitoring: 8222
    cluster: 6222
    metrics: 7777
    leafnodes: 7422
    gateways: 7522

  # Toggle profiling.
  # This enables nats-server pprof (profiling) port, so you can see goroutines
  # stacks, memory heap sizes, etc.
  profiling:
    enabled: false
    port: 6000

  # Toggle using health check probes to better detect failures.
  healthcheck:
    # Enable /healthz startupProbe for controlled upgrades of NATS JetStream
    enableHealthz: true

    # Enable liveness checks.  If this fails, then the NATS Server will restarted.
    liveness:
      enabled: true
      # `/healthz?js-enabled-only=true` will only check if NATS server is ready for accepting requests
      # and JetStream is enabled or not.
      # It will not check further the streams and consumers.
      # The endpoint to "/healthz?js-enabled-only=true" is supported in NATS >= v2.9.11
      endpoint: "/healthz?js-enabled-only=true"
      initialDelaySeconds: 10
      timeoutSeconds: 5
      # NOTE: liveness check + terminationGracePeriodSeconds can introduce unecessarily long outages
      # due to the coupling between liveness probe and terminationGracePeriodSeconds.
      # To avoid this, we make the periodSeconds of the liveness check to be about half the default
      # time that it takes for lame duck graceful stop.
      #
      # In case of using Kubernetes +1.22 with probe-level terminationGracePeriodSeconds
      # we could revise this but for now keep a minimal liveness check.
      #
      # More info:
      #
      #  https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#probe-level-terminationgraceperiodseconds
      #  https://github.com/kubernetes/kubernetes/issues/64715
      #
      periodSeconds: 30
      successThreshold: 1
      failureThreshold: 5
      # Only for Kubernetes +1.22 that have pod level probes enabled.
      terminationGracePeriodSeconds:

    # Periodically check for the server to be ready for connections while
    # the NATS container is running.
    # Disabled by default since covered by startup probe and it is the same
    # as the liveness check.
    readiness:
      enabled: true
      # `/healthz?js-server-only=true` will only check if JetStream server is current or not.
      # It will not check further the streams and consumers.
      endpoint: "/healthz?js-server-only=true"
      initialDelaySeconds: 10
      timeoutSeconds: 5
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 3

    # Enable startup checks to confirm server is ready for traffic.
    # This is recommended for JetStream deployments since in cluster mode
    # it will try to ensure that the server is ready to serve streams.
    startup:
      enabled: true
      # `/healthz` will check if JetStream server, streams and consumers are current or not.
      endpoint: "/healthz"
      initialDelaySeconds: 10
      timeoutSeconds: 5
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 90

  # Adds a hash of the ConfigMap as a pod annotation
  # This will cause the StatefulSet to roll when the ConfigMap is updated
  configChecksumAnnotation: false

  # Toggle to disable client advertisements (connect_urls),
  # in case of running behind a load balancer
  # it might be required to disable advertisements.
  advertise: true

  # The number of connect attempts against discovered routes.
  connectRetries: 120

  # selector matchLabels for the server and service.
  # If left empty defaults are used.
  # This is helpful if you are updating from Chart version <=7.4
  selectorLabels: {}

  resources:
    limits:
      cpu: 500m
      memory: 1Gi
    requests:
      cpu: 40m
      memory: 64Mi

  # Server settings.
  limits:
    maxConnections:
    maxSubscriptions:
    maxControlLine:
    maxPayload:

    writeDeadline:
    maxPending:
    maxPings:

    # How many seconds should pass before sending a PING
    # to a client that has no activity.
    pingInterval:

    # grace period after pod begins shutdown before starting to close client connections
    lameDuckGracePeriod: "10s"

    # duration over which to slowly close client connections after lameDuckGracePeriod has passed
    lameDuckDuration: "120s"

  # terminationGracePeriodSeconds determines how long to wait for graceful shutdown
  # this should be at least `lameDuckGracePeriod` + `lameDuckDuration` + 20s shutdown overhead
  terminationGracePeriodSeconds: 150

  logging:
    debug: false
    trace: false
    logtime:
    connectErrorReports:
    reconnectErrorReports:

  jetstream:
    # Jetstream Domain
    domain:

    # Jetstream Unique Tag prevent placing a stream in the same availability zone twice.
    uniqueTag:

    max_outstanding_catchup:

    ##########################
    #                        #
    #  Jetstream Encryption  #
    #                        #
    ##########################
    encryption:
      # Use key if you want to provide the key via Helm Values
      # key: random_key

      # Use a secret reference if you want to get a key from a secret
      # secret:
      #   name: "nats-jetstream-encryption"
      #   key: "key"

    #############################
    #                           #
    #  Jetstream Memory Storage #
    #                           #
    #############################
    memStorage:
      enabled: true
      size: 1Gi

    ############################
    #                          #
    #  Jetstream File Storage  #
    #                          #
    ############################
    fileStorage:
      #enabled: JetStream fileStorage can be enabled if .Values.global.jetstream.storage equals "file"
      storageDirectory: /data

      # Set for use with existing PVC
      # existingClaim: jetstream-pvc
      # claimStorageSize: 1Gi

      # Use below block to create new persistent volume
      # only used if existingClaim is not specified
      storageClassName: ""
      accessModes:
        - ReadWriteOnce
      annotations:
      # key: "value"

# Authentication setup
auth:
  enabled: true
  rotatePassword: true
  adminPassword:
  resolver:
    ##############################
    #                            #
    # Memory resolver settings   #
    #                            #
    ##############################
    type: memory


nameOverride: ""

# Affinity for pod assignment
# ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              nats_cluster: eventing-nats
          topologyKey: topology.kubernetes.io/zone
        weight: 70
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              nats_cluster: eventing-nats
          topologyKey: kubernetes.io/hostname
        weight: 35
# Annotations to add to the NATS pods
# ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
podAnnotations: {
  sidecar.istio.io/inject: "true"
}

# Annotations to add to the NATS StatefulSet
statefulSetAnnotations: {}

# Labels to add to the pods of the NATS StatefulSet
statefulSetPodLabels:
  nats_cluster: eventing-nats

# Annotations to add to the NATS Service
serviceAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "7777"

cluster:
  enabled: true
  name: eventing-nats
  replicas: 3
  noAdvertise: false

appProtocol:
  enabled: true

# Cluster Domain configured on the kubelets
# https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
k8sClusterDomain: cluster.local

# Add labels to all the deployed resources
commonLabels:
  app.kubernetes.io/component: nats-manager
  app.kubernetes.io/created-by: nats-manager
  app.kubernetes.io/managed-by: nats-manager
  app.kubernetes.io/part-of: nats-manager
  control-plane: nats-manager

# Add annotations to all the deployed resources
commonAnnotations: {}

# Prometheus NATS Exporter configuration.
exporter:
  enabled: true
  pullPolicy: IfNotPresent
  resources:
    limits:
      cpu: 50m
      memory: 32Mi
    requests:
      cpu: 10m
      memory: 20Mi

# NATS config reloader sidecar.
reloader:
  resources:
    limits:
      cpu: 50m
      memory: 32Mi
    requests:
      cpu: 10m
      memory: 20Mi
